import os 
import time 
import openai
import json
from typing import List
from datasets import load_dataset
from openai import OpenAI
from dataclasses import dataclass
# from together import Together
import datetime
from dotenv import load_dotenv

# L·∫•y ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c hi·ªán t·∫°i ch·ª©a script
current_dir = os.path.dirname(__file__)
env_path = os.path.join(current_dir, ".env")

load_dotenv(dotenv_path=env_path)

@dataclass
class BatchOpenAIConfig:
    model_name:str
    url:str
    dataset_name:str
    num_samples_range: tuple
    temperature: float
    top_p: float
    max_tokens: int
    column_name_list: List[str]
    system_prompt: str


class BatchProcessError(Exception):
    pass

class BatchOpenAIProcessor:

    """
    Args:
        client (OpenAI): The OpenAI client object.
        batch_openai_config (BatchOpenAIConfig): The configuration object for batch processing.
        
    Attributes:
        client (OpenAI): The OpenAI client object.
        batch_openai_config (BatchOpenAIConfig): The configuration object for batch processing.
        dataset (Dataset): The dataset object.
        sub_dataset (Dataset): The sub-dataset object.
        column_name_list (List[str]): The list of column names.
        system_prompt (str): The system prompt for the batch processing.
        url (str): The URL for the model endpoint.
        model_name (str): The name of the model.
        num_samples_range (tuple): The range of sample numbers for the batch processing.
        temperature (float): The temperature for the batch processing.
        top_p (float): The top_p for the batch processing.
        max_tokens (int): The max_tokens for the batch processing.

    """

    def __init__(self , client: OpenAI, batch_openai_config: BatchOpenAIConfig):

        self.client = client 
        self.batch_openai_config = batch_openai_config
        self.dataset = load_dataset(batch_openai_config.dataset_name, split="train")
        self.sub_dataset = self.dataset.select(range(batch_openai_config.num_samples_range[0], batch_openai_config.num_samples_range[1]))
        self.column_name_list = batch_openai_config.column_name_list

    def build_request(self, prompt_list, start_index, end_index):
        file_name = f'batch_input{start_index}_{end_index}.jsonl'
        with open(file_name, 'w', encoding='utf-8') as f:
            req_id = start_index * len(self.column_name_list)
            for item in prompt_list:
                for col in self.column_name_list:
                    prompt_text = f"{col}: {item[col]}"
                    record = {
                        "custom_id": f"req-{req_id}",
                        "method": "POST",
                        "url": "/v1/chat/completions",
                        "body": {
                            "model": self.batch_openai_config.model_name,
                            "messages": [
                                {"role": "system", "content": self.batch_openai_config.system_prompt},
                                {"role": "user", "content": prompt_text}
                            ]
                        }
                    }
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")
                    req_id += 1
        return file_name

    
    def make_json_list(self):
        prompt = []
        for item in self.sub_dataset:
            record = {}
            for col in self.column_name_list:
                record[col] = item[col]
            prompt.append(record)
        return prompt


    def generate_batch_response(self):
        # Step 1: Build request and write to input file
        prompt_list = self.make_json_list()
        start_idx, end_idx = self.batch_openai_config.num_samples_range
        input_file = self.build_request(prompt_list, start_index=start_idx, end_index=end_idx)
        self.batch_openai_config.input_file_path = input_file

        # Step 2: Upload input file
        with open(input_file, "rb") as f:
            upload_resp = self.client.files.create(file=f, purpose="batch")
        input_file_id = upload_resp.id
        print(f"Uploaded input file: {input_file} | File ID: {input_file_id}")

        # Step 3: Create batch
        batch_resp = self.client.batches.create(
            completion_window="24h",
            endpoint=self.batch_openai_config.url,
            input_file_id=input_file_id
        )
        batch_id = batch_resp.id
        print(f"Batch job created. Batch ID: {batch_id}")

        # Step 4: Save metadata
        meta = {
            "batch_id": batch_id,
            "input_file_id": input_file_id,
            "input_file_path": input_file,
            "timestamp": datetime.datetime.now().strftime("%Y%m%d_%H%M%S"),
            "status": "submitted"
        }
        meta_file = f"batch_meta_{batch_id}.json"
        with open(meta_file, "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2)
        print(f"Batch metadata saved to {meta_file}")

        # Step 5: Polling batch status
        print("Polling batch status...")
        status = batch_resp.status
        while status in ("validating", "in_progress", "finalizing"):
            print(f"Current status: {status}")
            time.sleep(5)
            batch_resp = self.client.batches.retrieve(batch_id)
            status = batch_resp.status

        print(f"Final status: {status}")

        # Step 6: Handle failure
        if status != "completed":
            print(f"Batch failed with status: {status}")
            if batch_resp.error_file_id:
                error_file_id = batch_resp.error_file_id
                print("Downloading error file...")
                error_content = self.client.files.content(error_file_id)
                error_path = f"batch_errors_{start_idx}_{end_idx}.jsonl"
                error_content.write_to_file(error_path)
                print(f"Error file saved to {error_path}")
                with open(error_path, "r", encoding="utf-8") as f:
                    print("First error line:")
                    print(f.readline())
            return False

        # Step 7: Download output
        output_file_id = batch_resp.output_file_id
        if not output_file_id:
            print("Batch completed but no output_file_id was returned.")
            return False

        print("Downloading result file...")
        output_jsonl_path = f"batch_results_{start_idx}_{end_idx}.jsonl"
        file_content = self.client.files.content(output_file_id)
        file_content.write_to_file(output_jsonl_path)
        print(f"Result file saved to {output_jsonl_path}")

        # Step 8: Convert to JSON
        self.merge_jsonl_files(output_jsonl_path)
        converted_json_path = f"converted_batch_results_{start_idx}_{end_idx}.json"

        # Step 9: Merge input and response
        self.merge_data(
            input_jsonl_path=input_file,
            response_json_path=converted_json_path,
            keys=self.batch_openai_config.column_name_list
        )

        print("Batch processing completed successfully.")


    def merge_jsonl_files(self, input_json_path:str ):
        data = []
        with open(input_json_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    obj = json.loads(line)
                    data.append(obj)
                except Exception as e:
                    print("Not a valid JSON line:", line)   
                
        
        output_name = 'converted_' + os.path.splitext(input_json_path)[0] + '.json'
        output_path = os.path.join(os.path.dirname(input_json_path), output_name)

        with open(output_path, 'w', encoding='utf-8') as out_f:
            json.dump(data, out_f, ensure_ascii=False, indent=2)

        print(f"Saving file: {output_path}")

    def merge_data(
        self,
        input_jsonl_path: str,
        response_json_path: str,
        keys: List[str],
        output_dir: str = None
    ):
        """
        Merge grouped request input data with model-generated responses based on `custom_id`.

        Args:
            input_jsonl_path (str): Path to the .jsonl input file. Each group of N lines corresponds to a single logical unit.
            response_json_path (str): Path to the JSON file containing responses from the model, each linked by custom_id.
            keys (List[str]): List of logical keys corresponding to each line in a group (e.g. ["Question", "Reasoning", "Answer"]).
            output_dir (str, optional): Directory to save the merged output file. Defaults to the input file's directory.

        Output:
            A single JSON file with merged input and translated responses for each group.
        """
        group_size = len(keys)

        # Load input .jsonl file (each group_size lines = 1 logical unit)
        with open(input_jsonl_path, "r", encoding="utf-8") as f:
            input_entries = [json.loads(line) for line in f]

        # Load response JSON (structured as a list of responses, each with custom_id)
        with open(response_json_path, "r", encoding="utf-8") as f:
            response_data = json.load(f)

        # Group input entries into logical units
        grouped_inputs = []
        for i in range(0, len(input_entries), group_size):
            group = {}
            for j, key in enumerate(keys):
                group[key] = input_entries[i + j]["body"]["messages"][-1]["content"]
            group["custom_id"] = input_entries[i]["custom_id"]
            grouped_inputs.append(group)

        # Create lookup map from custom_id to response text
        response_lookup = {
            item["custom_id"]: item["response"]["body"]["choices"][0]["message"]["content"]
            for item in response_data
        }

        # Merge input group with corresponding responses
        merged_result = []
        for i in range(0, len(response_data), group_size):
            ids = [response_data[i + j]["custom_id"] for j in range(group_size)]
            matched = next((x for x in grouped_inputs if x["custom_id"] == ids[0]), None)

            if matched:
                for j, key in enumerate(keys):
                    matched[f"{key}_response"] = response_lookup.get(ids[j], "")
                merged_result.append(matched)

        # Determine output path
        if output_dir is None:
            output_dir = os.path.dirname(input_jsonl_path)

        output_name = f"merged_output_{os.path.basename(input_jsonl_path).replace('.jsonl', '')}.json"
        output_path = os.path.join(output_dir, output_name)

        # Write to file
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(merged_result, f, ensure_ascii=False, indent=2)

        print(f"Merged file saved to: {output_path}")

if __name__ == '__main__':
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    # Load to√†n b·ªô dataset
    full_dataset = load_dataset("trungnguyen2331/law_extract", split="train")
    total_samples = len(full_dataset)
    batch_size = 10
    start_from = 301

    # L·∫∑p qua t·ª´ng batch t·ª´ index 1001
    for start_idx in range(start_from, total_samples, batch_size):
        end_idx = min(start_idx + batch_size, total_samples)
        print(f"\nüöÄ ƒêang x·ª≠ l√Ω batch t·ª´ {start_idx} ƒë·∫øn {end_idx}...\n")

        config = BatchOpenAIConfig(
            model_name=os.getenv("OPENAI_MODEL_NAME"),
            url="/v1/chat/completions",
            dataset_name="trungnguyen2331/law_extract",
            num_samples_range=(start_idx, end_idx),
            temperature=0.7,
            top_p=0.95,
            max_tokens=10000,
            column_name_list=['context'],
            system_prompt=r"""
B·∫°n l√† m·ªôt chuy√™n gia ph√°p lu·∫≠t c√≥ nhi·ªám v·ª• **tr√≠ch xu·∫•t th√¥ng tin c√≥ c·∫•u tr√∫c** t·ª´ vƒÉn b·∫£n ph√°p lu·∫≠t ƒë√£ ƒë∆∞·ª£c s·ªë h√≥a (OCR ho·∫∑c ƒë·ªãnh d·∫°ng vƒÉn b·∫£n th∆∞·ªùng).

Y√™u c·∫ßu b·∫Øt bu·ªôc:
- D·ª±a v√†o ph·∫ßn `context` b√™n d∆∞·ªõi, h√£y ƒëi·ªÅn d·ªØ li·ªáu v√†o **b·∫£ng JSON** ƒë√∫ng theo ƒë·ªãnh d·∫°ng v√† t√™n tr∆∞·ªùng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh.
- N·∫øu **thi·∫øu qu√° nhi·ªÅu tr∆∞·ªùng th√¥ng tin quan tr·ªçng**, h√£y tr·∫£ v·ªÅ chu·ªói duy nh·∫•t:
  `PDF kh√¥ng ch·ª©a ƒë·ªß th√¥ng tin ƒë·ªÉ ƒëi·ªÅn v√†o b·∫£ng.`
- Tr·∫£ v·ªÅ ƒë·ªëi t∆∞·ª£ng JSON **g·ªëc, kh√¥ng b·ªçc trong chu·ªói**.
- Tr∆∞·ªùng `"noi_dung"` ph·∫£i ch·ª©a **to√†n b·ªô n·ªôi dung vƒÉn b·∫£n t·ª´ ph·∫ßn **Qu·ªëc hi·ªáu Ti√™u ng·ªØ tr·ªü xu·ªëng**, kh√¥ng ƒë∆∞·ª£c r√∫t g·ªçn ho·∫∑c m√¥ t·∫£ b·∫±ng l·ªùi.
- Ch·ªâ tr·∫£ v·ªÅ ƒë√∫ng m·ªôt trong hai:
  1. M·ªôt ƒë·ªëi t∆∞·ª£ng JSON ƒë·∫ßy ƒë·ªß.
  2. Ho·∫∑c chu·ªói `"PDF kh√¥ng ch·ª©a ƒë·ªß th√¥ng tin ƒë·ªÉ ƒëi·ªÅn v√†o b·∫£ng."`

---

C·∫•u tr√∫c JSON y√™u c·∫ßu:
```json
{
  "so_hieu": "...",                  // S·ªë hi·ªáu vƒÉn b·∫£n (VD: 01/2023/TT-BGDƒêT)
  "loai_vb": "...",                  // Lo·∫°i vƒÉn b·∫£n (VD: Th√¥ng t∆∞, Ngh·ªã ƒë·ªãnh)
  "noi_ban_hanh": "...",            // C∆° quan ban h√†nh (VD: B·ªô T√†i ch√≠nh)
  "nguoi_ky": "...",                // Ng∆∞·ªùi k√Ω ban h√†nh
  "ngay_ban_hanh": "...",           // Ng√†y ban h√†nh (YYYY-MM-DD)
  "ngay_hieu_luc": "...",           // Ng√†y c√≥ hi·ªáu l·ª±c
  "ngay_cong_bao": "...",           // Ng√†y c√¥ng b·ªë
  "so_cong_bao": "...",             // S·ªë C√¥ng b√°o
  "tinh_trang": "...",              // Tr·∫°ng th√°i hi·ªáu l·ª±c (VD: C√≤n hi·ªáu l·ª±c)
  "tieu_de": "...",                 // T√™n vƒÉn b·∫£n
  "noi_dung": "...",                // To√†n b·ªô n·ªôi dung c·ªßa vƒÉn b·∫£n, kh√¥ng t√≥m t·∫Øt
  "linh_vuc": "..."                 // Lƒ©nh v·ª±c (VD: Gi√°o d·ª•c, Y t·∫ø)
}
"H√£y tr√≠ch xu·∫•t th√¥ng tin theo y√™u c·∫ßu."
"""
        )

        processor = BatchOpenAIProcessor(client=client, batch_openai_config=config)
        try:
            processor.generate_batch_response()
        except Exception as e:
            print(f"‚ùå Batch {start_idx}-{end_idx} l·ªói: {e}")